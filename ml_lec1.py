# машинное обучение
# ИИ-> машинное-> нейросетки -> глубокое обучение

# ML - решает следующую задачу
# Требуется подогнать заданный набор точек данными под соответствующую функцию ( отображение входа на выход), которая улавливает важные
# сигналы в данных и игнорирует помехи, а затем убедиться, что на новых данных функция работает хорошо
#
#Обучение с учителем (supervised learning)
#Обучение без учителя (unsupervised learning)
#
#ОчУ - модель отношения между признаками и метками. Такие модели служат для предсказания меток на основе обучающих данных маркированных.
#После построенния модели можно использовать её для присвоения меток новым ранее неизвестным данным.
#
#-задача классификации (метки - дискретные: два или более)
#-задача регрессии (метки.результат: непрерывные величины)
#
#
# ОбУ - моделирование признаков без меток. Такие модели служат для выявления структуры немаркированных данных
#
# - задача кластеризации (выделяет отдельнве группы данных)
# - понижения размерности (поиск более сжатого представления данных)
#
# Существует методы частичного обучения (semi-supervised learning). Не все данные промаркерованы
#
# Методы обучения с подкреплением ()reinforcement learing). Система обучения улучшает свои характеристики на основе взаимодействия ( обратной связи) со средой. при этом взаимодействии система получает сигналы (функции наград), которые несут в себе информацию насколько хорошо\плохо
# система решила задачу ( с точки зрения среды). Итоговая награда не станет максимальной.


import seaborn as sns

iris = sns.load_dataset('iris')
print(iris.head())
print(type(iris))
print(type(iris.values))

print(iris.values.shape)

print(iris.columns)

print(iris.index)

# Строки - отдельные объекты - образцы (sample)
# Столбца - признаки (features) - соответствуют конкретным наблюдением
# Матрица признаком (features matrix) размер [число образцов x число признаков]
# Целевой массив, массив меток (targets) - одномерный массив [1 х число образцов] - данные, которые мы хотим предсказать на основе имеющихся данных
# зависимые (метка) и независимые переменные (признаки)


#Процесс построения системы машинного обучения:

#1. Предварительная обработка
# - На вход поступают необработынные данные и метки
# - Происходит выбор признаков, масщтабирование признаков
# - Понижение размерности
# - Выборка образцов
# - На выход набор данных: обучающий набор и тестовый


# 2. Обучение
# - Выбор модели
# - Прекрестная проверка
# - Метрики эффективности
# - Оптимизаци гиперпараметров. Параметры, которые получаются не из данных, а являются характеристикам модели


#3. Оценка и формирование финальной модели


# 4. Прогнозирование ( использование модели)


#SciKit - learn
#
#1. Выбираем класс модели
# 2. Устанавливаем гиперпараметрв модели
# 3. Создаем матрицу признаков и целевой массив
# 4. Обучение модели fit()
# 5. Применять модель к новым данным
# - predict() (с учителем)
# - predict() или transform() (без учителя)


# Обучение с учителем: Линейная регрессия

## Простой линейной регресии

# y = ax+b, x- признак, y-  метка
import matplotlib.pyplot as plt
import numpy as np

#II
np.random.seed(1)
x=10*np.random.rand(50)
y=2*x-1 +np.random.randn(50)
plt.scatter(x,y)
print(x.shape)
print(y.shape)
X=x[:, np.newaxis]
#II
#1. Выбираем класс модели
from sklearn.linear_model import LinearRegression

# 2. Устанавливаем гиперпараметры модели
model = LinearRegression(fit_intercept=False)

# 3. Создаем матрицу признаков и целевой массив


# 4. Обучение модели fit()
model.fit(X,y)
print(model.coef_[0])
print(model.intercept_)

x_=np.linspace(0,10,30)
y_=model.coef_[0] * x_ + model.intercept_
plt.plot(x_, y_)

# 5. Применять модель к новым данным
xfit = np.linspace(-10,10,5)
yfit = model.predict(xfit[:,np.newaxis])
plt.scatter(xfit, yfit)
# - predict() (с учителем)
# - predict() или transform() (без учителя)




plt.show()
#
#




